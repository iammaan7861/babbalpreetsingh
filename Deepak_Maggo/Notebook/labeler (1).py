# -*- coding: utf-8 -*-
"""labeler.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_MbB0A9GejkuMN6ZgYKwdH0VNU_cvsM
"""

# Installs the necessary Kaggle libraries
!pip install kaggle kagglehub -q

# Imports the libraries to securely access the keys you just added
import os
from google.colab import userdata

# Sets up authentication with Kaggle
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')
os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')

# Imports the main tool for downloading
import kagglehub

# Starts the download process
print("üöÄ Starting dataset download...")
path = kagglehub.dataset_download("sshikamaru/udacity-self-driving-car-dataset")

# Confirms completion and shows you where the files are
print("\n‚úÖ Download complete!")
print("Your dataset is located at this path:", path)



import os
import random
import shutil

# The path where your dataset was downloaded
source_dir = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2/object-detection-crowdai'

# The new directory we will create to hold our organized dataset
base_dir = '/content/final_dataset'

# The specific subdirectories for training and testing images
train_dir = os.path.join(base_dir, 'train', 'images')
test_dir = os.path.join(base_dir, 'test', 'images')

# Create the directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

print(f"üìÇ Created directories at {base_dir}")

# Get a list of all image files
all_images = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
random.shuffle(all_images) # Shuffle the list to ensure randomness

# --- Select and Copy Images ---
# We'll select 100 for training and 50 for testing, as required.

# Select the first 100 images for the training set
train_images = all_images[:100]
for image_name in train_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(train_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(train_images)} images to the training set.")

# Select the next 50 images for the test set
test_images = all_images[100:150]
for image_name in test_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(test_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(test_images)} images to the test set.")
print("\n‚úÖ Dataset organization is complete!")

import os

# This is the base path that we know is correct
base_path = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2'

# Let's list what's inside this folder
print("Found folder(s):", os.listdir(base_path))

import os
import random
import shutil

# The base path of the downloaded dataset
base_path = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2'

# ----------------- YOUR ACTION HERE -----------------
# Replace 'PASTE_FOLDER_NAME_HERE' with the folder name from the output above
correct_folder_name = 'PASTE_FOLDER_NAME_HERE'
# ----------------------------------------------------

# The corrected full path to the images
source_dir = os.path.join(base_path, correct_folder_name)


# --- The rest of the script is the same ---
base_dir = '/content/final_dataset'
train_dir = os.path.join(base_dir, 'train', 'images')
test_dir = os.path.join(base_dir, 'test', 'images')

os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)
print(f"üìÇ Created directories at {base_dir}")

# Get a list of all image files from the corrected path
all_images = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
random.shuffle(all_images)

# Select and Copy Images
train_images = all_images[:100]
for image_name in train_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(train_dir, image_name)
    shutil.copy(source_path, destination_path)
print(f"üöÄ Copied {len(train_images)} images to the training set.")

test_images = all_images[100:150]
for image_name in test_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(test_dir, image_name)
    shutil.copy(source_path, destination_path)
print(f"üöÄ Copied {len(test_images)} images to the test set.")

print("\n‚úÖ Dataset organization is complete!")

import os
import random
import shutil

# The corrected folder name 'data' is now included in the path.
source_dir = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2/data'

# The new directory we will create to hold our organized dataset
base_dir = '/content/final_dataset'

# The specific subdirectories for training and testing images
train_dir = os.path.join(base_dir, 'train', 'images')
test_dir = os.path.join(base_dir, 'test', 'images')

# Create the directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)
print(f"üìÇ Created directories at {base_dir}")

# Get a list of all image files from the corrected path
all_images = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
random.shuffle(all_images) # Shuffle the list to ensure randomness

# --- Select and Copy Images ---
# We'll select 100 for training and 50 for testing, as required.

# Select the first 100 images for the training set
train_images = all_images[:100]
for image_name in train_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(train_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(train_images)} images to the training set.")

# Select the next 50 images for the test set
test_images = all_images[100:150]
for image_name in test_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(test_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(test_images)} images to the test set.")
print("\n‚úÖ Dataset organization is complete!")

import os

source_dir = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2/data'

# Let's see what's inside this directory
try:
    contents = os.listdir(source_dir)
    print(f"Found {len(contents)} items in the 'data' folder.")
    print("Here are the first 10 items:")
    print(contents[:10]) # Print the first 10 items to avoid a huge list
except FileNotFoundError:
    print("Error: The source directory was not found. Please re-run the download step.")

import os
import random
import shutil

# This is the final, correct path to the images.
source_dir = '/root/.cache/kagglehub/datasets/sshikamaru/udacity-self-driving-car-dataset/versions/2/data/export'

# The new directory we will create to hold our organized dataset
base_dir = '/content/final_dataset'

# The specific subdirectories for training and testing images
train_dir = os.path.join(base_dir, 'train', 'images')
test_dir = os.path.join(base_dir, 'test', 'images')

# Create the directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)
print(f"üìÇ Created directories at {base_dir}")

# Get a list of all image files from the corrected path
all_images = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
random.shuffle(all_images) # Shuffle the list to ensure randomness

# --- Select and Copy Images ---
# We'll select 100 for training and 50 for testing, as required.

# Select the first 100 images for the training set
train_images = all_images[:100]
for image_name in train_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(train_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(train_images)} images to the training set.")

# Select the next 50 images for the test set
test_images = all_images[100:150]
for image_name in test_images:
    source_path = os.path.join(source_dir, image_name)
    destination_path = os.path.join(test_dir, image_name)
    shutil.copy(source_path, destination_path)

print(f"üöÄ Copied {len(test_images)} images to the test set.")
print("\n‚úÖ Dataset organization is complete!")

import shutil
from google.colab import files

# The folder we want to zip
folder_to_zip = '/content/final_dataset'

# The name of the output zip file
output_zip_file = '/content/final_dataset.zip'

# Zipping the folder
shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', folder_to_zip)

print(f"üì¶ Folder zipped successfully to {output_zip_file}")
print("Starting download...")

# Triggering the download
files.download(output_zip_file)

from google.colab import files
import os

print("Please upload the dataset ZIP file you exported from Labellerr.")
uploaded = files.upload()

# Get the name of the uploaded zip file
zip_name = next(iter(uploaded))

# Unzip the file into a new 'dataset' directory
!unzip -q {zip_name} -d /content/dataset

print("\n‚úÖ Dataset uploaded and unzipped successfully!")

# Step 1: Install Ultralytics YOLOv8
!pip install ultralytics -q

from ultralytics import YOLO
import os

# Step 2: Create the dataset configuration file (data.yaml)
# IMPORTANT: This assumes your classes are named 'vehicle' and 'pedestrian'.
# If you named them differently in Labellerr, you MUST change the 'names' list below.
yaml_content = """
train: /content/dataset/train/images
val: /content/... # The zip from Labellerr might have a 'valid' folder, if not, we use train again.

# Number of classes
nc: 2

# Class names
names: ['vehicle', 'pedestrian']
"""

# Check if a validation set exists, if not, YOLO will auto-split from train
val_path = '/content/dataset/valid/images'
if not os.path.exists(val_path):
    # Remove the 'val' line if the folder doesn't exist
    yaml_content = yaml_content.replace(f"val: {val_path}\n", "")


with open('data.yaml', 'w') as f:
    f.write(yaml_content)

# Step 3: Train the YOLOv8-Segmentation model
# Load a pretrained model to start from
model = YOLO('yolov8n-seg.pt') # Using the small 'nano' segmentation model for speed

# Train the model üß†
print("Starting model training... This will take some time.")
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    project='Labellerr_Assignment', # Saves results to a folder named 'Labellerr_Assignment'
    name='training_results'
)

print("\nüéâ Training complete! Your trained model is saved.")

# Step 1: Install Ultralytics YOLOv8 (if not already done)
!pip install ultralytics -q

from ultralytics import YOLO
import os

# --- CORRECTED YAML CREATION ---
# Define the paths to your training and validation image folders
train_path = '/content/dataset/train/images'
val_path = '/content/dataset/valid/images'

# Start building the YAML content
# IMPORTANT: This assumes your classes are named 'vehicle' and 'pedestrian'.
# If you named them differently in Labellerr, you MUST change the 'names' list below.
yaml_content = f"""
train: {train_path}
"""

# Check if the validation directory exists. If it does, add it to the YAML.
# If not, YOLO will automatically split the training data for validation.
if os.path.exists(val_path):
    yaml_content += f"\nval: {val_path}"

# Add the number of classes and class names
yaml_content += """

# Number of classes
nc: 2

# Class names
names: ['vehicle', 'pedestrian']
"""

# Write the corrected content to the data.yaml file
with open('data.yaml', 'w') as f:
    f.write(yaml_content)

print("‚úÖ Corrected data.yaml file created.")
print("--- YAML Content ---")
print(yaml_content)
print("--------------------")

# Step 3: Train the YOLOv8-Segmentation model
# Load a pretrained model to start from
model = YOLO('yolov8n-seg.pt') # Using the small 'nano' segmentation model for speed

# Train the model üß†
print("\nüöÄ Starting model training... This will take some time.")
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    project='Labellerr_Assignment', # Saves results to a folder named 'Labellerr_Assignment'
    name='training_results',
    device='cpu' # Explicitly set to CPU if no GPU is available
)

print("\nüéâ Training complete! Your trained model is saved.")

import os
import random
import shutil

# --- Step 1: Automatically create a validation set ---

# Define paths for the original training data
source_train_images = '/content/dataset/train/images'
source_train_labels = '/content/dataset/train/labels'

# Define paths for the new validation set we will create
dest_valid_images = '/content/dataset/valid/images'
dest_valid_labels = '/content/dataset/valid/labels'

# Create the new validation directories
os.makedirs(dest_valid_images, exist_ok=True)
os.makedirs(dest_valid_labels, exist_ok=True)

# Get a list of all training images
images_to_move = os.listdir(source_train_images)
random.shuffle(images_to_move)

# We will move 20 images to create our validation set
num_to_move = 20
images_subset = images_to_move[:num_to_move]

print(f"Moving {len(images_subset)} images and labels to the validation set...")

# Loop through the subset and move the image AND its corresponding label file
for image_name in images_subset:
    # Get the base name of the file without extension
    base_filename = os.path.splitext(image_name)[0]
    label_name = f"{base_filename}.txt"

    # Construct the full source and destination paths
    src_image_path = os.path.join(source_train_images, image_name)
    dest_image_path = os.path.join(dest_valid_images, image_name)
    src_label_path = os.path.join(source_train_labels, label_name)
    dest_label_path = os.path.join(dest_valid_labels, label_name)

    # Move the files
    if os.path.exists(src_label_path):
        shutil.move(src_image_path, dest_image_path)
        shutil.move(src_label_path, dest_label_path)

print("‚úÖ Validation set created successfully.")

# --- Step 2: Install Ultralytics and Create Correct YAML ---

!pip install ultralytics -q
from ultralytics import YOLO

# Now we can create the YAML file, as both paths are guaranteed to exist
yaml_content = f"""
train: {source_train_images}
val: {dest_valid_images}

# Number of classes
nc: 2

# Class names
names: ['vehicle', 'pedestrian']
"""

with open('data.yaml', 'w') as f:
    f.write(yaml_content)

print("‚úÖ data.yaml file created successfully.")
print("--- YAML Content ---")
print(yaml_content)
print("--------------------")

# --- Step 3: Train the Model ---
model = YOLO('yolov8n-seg.pt')

print("\nüöÄ Starting model training... This will take some time.")
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    project='Labellerr_Assignment',
    name='training_results',
    device='cpu'
)

print("\nüéâ Training complete! Your trained model is saved.")

!ls -R /content/dataset

import os
import random
import shutil

# --- Step 1: Organize the messy files into a proper train/valid structure ---

# This is the source directory where Labellerr dumped all files
source_dir = '/content/dataset'

# These are the clean directories we will create and move files into
train_images_dir = '/content/final_dataset/train/images'
train_labels_dir = '/content/final_dataset/train/labels'
valid_images_dir = '/content/final_dataset/valid/images'
valid_labels_dir = '/content/final_dataset/valid/labels'

# Create all the necessary directories
os.makedirs(train_images_dir, exist_ok=True)
os.makedirs(train_labels_dir, exist_ok=True)
os.makedirs(valid_images_dir, exist_ok=True)
os.makedirs(valid_labels_dir, exist_ok=True)

# Get a list of all IMAGE files from the messy source directory
all_images = [f for f in os.listdir(source_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]
random.shuffle(all_images)

# Let's do an 80/20 split. 80% for training, 20% for validation.
split_index = int(0.8 * len(all_images))
train_image_files = all_images[:split_index]
valid_image_files = all_images[split_index:]

print(f"Organizing files: {len(train_image_files)} for training, {len(valid_image_files)} for validation.")

# Function to move an image and its corresponding label file
def move_files(file_list, dest_img_dir, dest_label_dir):
    for image_name in file_list:
        base_filename = os.path.splitext(image_name)[0]
        label_name = f"{base_filename}.txt"

        # Source paths
        src_image_path = os.path.join(source_dir, image_name)
        src_label_path = os.path.join(source_dir, label_name)

        # Destination paths
        dest_image_path = os.path.join(dest_img_dir, image_name)
        dest_label_path = os.path.join(dest_label_dir, label_name)

        # Move the image and its label if they both exist
        if os.path.exists(src_image_path) and os.path.exists(src_label_path):
            shutil.move(src_image_path, dest_image_path)
            shutil.move(src_label_path, dest_label_path)

# Move the files to their new homes
move_files(train_image_files, train_images_dir, train_labels_dir)
move_files(valid_image_files, valid_images_dir, valid_labels_dir)

print("‚úÖ File organization complete.")

# --- Step 2: Install Ultralytics and Create Correct YAML ---

!pip install ultralytics -q
from ultralytics import YOLO

# Create the YAML file, pointing to our newly organized folders
yaml_content = f"""
train: {train_images_dir}
val: {valid_images_dir}

# Number of classes
nc: 2

# Class names
names: ['vehicle', 'pedestrian']
"""

with open('data.yaml', 'w') as f:
    f.write(yaml_content)

print("‚úÖ data.yaml file created successfully.")

# --- Step 3: Train the Model ---
model = YOLO('yolov8n-seg.pt')

print("\nüöÄ Starting model training... This will take some time.")
results = model.train(
    data='data.yaml',
    epochs=100,
    imgsz=640,
    project='Labellerr_Assignment',
    name='training_results',
    device='cpu' # Using CPU as per the logs
)

print("\nüéâ Training complete! Your trained model is saved.")

import os

source_dir = '/content/dataset'

try:
    # Find all files that end with .jpg (case-insensitive)
    image_files = [f for f in os.listdir(source_dir) if f.lower().endswith('.jpg')]

    print(f"Found {len(image_files)} '.jpg' image files.")

    if image_files:
        print("\nHere are the first 5 image filenames:")
        for name in image_files[:5]:
            print(name)

except FileNotFoundError:
    print(f"Error: The directory '{source_dir}' was not found.")

import os

source_dir = '/content/dataset'
image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif'}
label_extensions = {'.txt'}

image_count = 0
label_count = 0
other_files = []

try:
    all_files = os.listdir(source_dir)
    for f in all_files:
        ext = os.path.splitext(f)[1].lower()
        if ext in image_extensions:
            image_count += 1
        elif ext in label_extensions:
            label_count += 1
        else:
            other_files.append(f)

    print(f"--- Directory Analysis ---")
    print(f"üñºÔ∏è Found {image_count} image files.")
    print(f"üìù Found {label_count} label (.txt) files.")
    print(f"üìÅ Found {len(other_files)} other files.")

    if other_files:
        print("\nExamples of other files found:")
        for name in other_files[:5]:
            print(name)

except FileNotFoundError:
    print(f"Error: The directory '{source_dir}' was not found.")

# Install the Labellerr SDK
!pip install labellerr-sdk -q

import labellerr
import os
from google.colab import userdata
import time
import requests

print("üöÄ Starting SDK process...")

# --- Authenticate with Labellerr ---
try:
    API_KEY = userdata.get('LABELLERR_API_KEY')
    client = labellerr.login(api_key=API_KEY)
    print("‚úÖ Successfully authenticated with Labellerr.")
except Exception as e:
    print(f"‚ùå Authentication failed. Please double-check your API key in Colab Secrets. Error: {e}")
    # Stop execution if authentication fails
    raise SystemExit()

# --- Export the Project ---
# From your previous screenshot, this is your Project ID.
# If you have a different project, you will need to change this.
PROJECT_ID = 'maisey_costly_bee_90820'
EXPORT_FORMAT = 'YOLO'
EXPORT_NAME = 'sdk_export'

print(f"Triggering export for project: {PROJECT_ID}")
try:
    # This command tells Labellerr to start preparing the export
    export_job = client.export.create_export(
        project_id=PROJECT_ID,
        export_name=EXPORT_NAME,
        export_format=EXPORT_FORMAT,
        export_type='all',
        include_source_data=True # This is the crucial part
    )

    export_id = export_job['report_id']
    print(f"‚è≥ Export job started with ID: {export_id}. Waiting for it to complete...")

    # --- Wait for the Export to Finish ---
    # We will check the status every 10 seconds until it's ready.
    download_url = None
    for _ in range(30): # Wait for a maximum of 5 minutes (30 * 10 seconds)
        status_check = client.export.get_export_by_id(export_id)
        current_status = status_check['status']
        print(f"Current status: {current_status}")
        if current_status.lower() == 'created':
            download_url = status_check['download_path']
            print("‚úÖ Export complete!")
            break
        time.sleep(10)

    # --- Download and Unzip the File ---
    if download_url:
        print(f"Downloading file from: {download_url}")
        response = requests.get(download_url)
        zip_filename = 'sdk_exported_data.zip'
        with open(zip_filename, 'wb') as f:
            f.write(response.content)

        # Delete old dataset folders if they exist
        !rm -rf /content/dataset
        !rm -rf /content/final_dataset

        # Unzip the new file
        !unzip -q {zip_filename} -d /content/dataset

        print("\nüéâ Dataset successfully downloaded and unzipped via SDK!")
    else:
        print("‚ùå Export job did not complete in time or failed.")

except Exception as e:
    print(f"‚ùå An error occurred during the export process: {e}")

# CORRECTED: The package name is 'labellerr'
!pip install labellerr -q

import labellerr
import os
from google.colab import userdata
import time
import requests

print("üöÄ Starting SDK process...")

# --- Authenticate with Labellerr ---
try:
    API_KEY = userdata.get('LABELLERR_API_KEY')
    client = labellerr.login(api_key=API_KEY)
    print("‚úÖ Successfully authenticated with Labellerr.")
except Exception as e:
    print(f"‚ùå Authentication failed. Please double-check your API key in Colab Secrets. Error: {e}")
    # Stop execution if authentication fails
    raise SystemExit()

# --- Export the Project ---
# From your previous screenshot, this is your Project ID.
PROJECT_ID = 'maisey_costly_bee_90820'
EXPORT_FORMAT = 'YOLO'
EXPORT_NAME = 'sdk_export'

print(f"Triggering export for project: {PROJECT_ID}")
try:
    # This command tells Labellerr to start preparing the export
    export_job = client.export.create_export(
        project_id=PROJECT_ID,
        export_name=EXPORT_NAME,
        export_format=EXPORT_FORMAT,
        export_type='all',
        include_source_data=True
    )

    export_id = export_job['report_id']
    print(f"‚è≥ Export job started with ID: {export_id}. Waiting for it to complete...")

    # --- Wait for the Export to Finish ---
    download_url = None
    for _ in range(30): # Wait for a maximum of 5 minutes (30 * 10 seconds)
        status_check = client.export.get_export_by_id(export_id)
        current_status = status_check['status']
        print(f"Current status: {current_status}")
        if current_status.lower() == 'created':
            download_url = status_check['download_path']
            print("‚úÖ Export complete!")
            break
        time.sleep(10)

    # --- Download and Unzip the File ---
    if download_url:
        print(f"Downloading file from: {download_url}")
        response = requests.get(download_url)
        zip_filename = 'sdk_exported_data.zip'
        with open(zip_filename, 'wb') as f:
            f.write(response.content)

        # Delete old dataset folders if they exist
        !rm -rf /content/dataset
        !rm -rf /content/final_dataset

        # Unzip the new file
        !unzip -q {zip_filename} -d /content/dataset

        print("\nüéâ Dataset successfully downloaded and unzipped via SDK!")
    else:
        print("‚ùå Export job did not complete in time or failed.")

except Exception as e:
    print(f"‚ùå An error occurred during the export process: {e}")



# CORRECTED SPELLING: 'labeller' with one 'r'
!pip install labeller -q

# CORRECTED SPELLING: 'labeller' with one 'r'
import labeller
import os
from google.colab import userdata
import time
import requests

print("üöÄ Starting SDK process...")

# --- Authenticate with Labellerr ---
try:
    API_KEY = userdata.get('LABELLERR_API_KEY')
    # CORRECTED SPELLING: 'labeller' with one 'r'
    client = labeller.login(api_key=API_KEY)
    print("‚úÖ Successfully authenticated with Labellerr.")
except Exception as e:
    print(f"‚ùå Authentication failed. Please double-check your API key in Colab Secrets. Error: {e}")
    raise SystemExit()

# --- Export the Project ---
PROJECT_ID = 'maisey_costly_bee_90820'
EXPORT_FORMAT = 'YOLO'
EXPORT_NAME = 'sdk_export'

print(f"Triggering export for project: {PROJECT_ID}")
try:
    export_job = client.export.create_export(
        project_id=PROJECT_ID,
        export_name=EXPORT_NAME,
        export_format=EXPORT_FORMAT,
        export_type='all',
        include_source_data=True
    )

    export_id = export_job['report_id']
    print(f"‚è≥ Export job started with ID: {export_id}. Waiting for it to complete...")

    # --- Wait for the Export to Finish ---
    download_url = None
    for _ in range(30): # Wait for a maximum of 5 minutes
        status_check = client.export.get_export_by_id(export_id)
        current_status = status_check['status']
        print(f"Current status: {current_status}")
        if current_status.lower() == 'created':
            download_url = status_check['download_path']
            print("‚úÖ Export complete!")
            break
        time.sleep(10)

    # --- Download and Unzip the File ---
    if download_url:
        print(f"Downloading file from: {download_url}")
        response = requests.get(download_url)
        zip_filename = 'sdk_exported_data.zip'
        with open(zip_filename, 'wb') as f:
            f.write(response.content)

        !rm -rf /content/dataset
        !rm -rf /content/final_dataset

        !unzip -q {zip_filename} -d /content/dataset

        print("\nüéâ Dataset successfully downloaded and unzipped via SDK!")
    else:
        print("‚ùå Export job did not complete in time or failed.")

except Exception as e:
    print(f"‚ùå An error occurred during the export process: {e}")

!pip install labeller -q

import labeller
import os
from google.colab import userdata
import time
import requests

print("üöÄ Starting SDK process...")

# --- CORRECTED Authentication ---
try:
    API_KEY = userdata.get('LABELLERR_API_KEY')
    # The correct method is to create a Client instance
    client = labeller.Client(api_key=API_KEY)
    print("‚úÖ Successfully authenticated with Labellerr.")
except Exception as e:
    print(f"‚ùå Authentication failed. Please double-check your API key in Colab Secrets. Error: {e}")
    raise SystemExit()

# --- Export the Project ---
PROJECT_ID = 'maisey_costly_bee_90820'
EXPORT_FORMAT = 'YOLO'
EXPORT_NAME = 'sdk_export'

print(f"Triggering export for project: {PROJECT_ID}")
try:
    export_job = client.export.create_export(
        project_id=PROJECT_ID,
        export_name=EXPORT_NAME,
        export_format=EXPORT_FORMAT,
        export_type='all',
        include_source_data=True
    )

    export_id = export_job['report_id']
    print(f"‚è≥ Export job started with ID: {export_id}. Waiting for it to complete...")

    # --- Wait for the Export to Finish ---
    download_url = None
    for _ in range(30): # Wait for a maximum of 5 minutes
        status_check = client.export.get_export_by_id(export_id)
        current_status = status_check['status']
        print(f"Current status: {current_status}")
        if current_status.lower() == 'created':
            download_url = status_check['download_path']
            print("‚úÖ Export complete!")
            break
        time.sleep(10)

    # --- Download and Unzip the File ---
    if download_url:
        print(f"Downloading file from: {download_url}")
        response = requests.get(download_url)
        zip_filename = 'sdk_exported_data.zip'
        with open(zip_filename, 'wb') as f:
            f.write(response.content)

        !rm -rf /content/dataset
        !rm -rf /content/final_dataset

        !unzip -q {zip_filename} -d /content/dataset

        print("\nüéâ Dataset successfully downloaded and unzipped via SDK!")
    else:
        print("‚ùå Export job did not complete in time or failed.")

except Exception as e:
    print(f"‚ùå An error occurred during the export process: {e}")

# Install the library
!pip install labeller -q

import labeller

print("--- Inspecting the 'labeller' library ---")

# Use the dir() function to list all available attributes (functions, classes, etc.)
# We'll filter out the standard Python '__' attributes to make the list clean.
available_attributes = [attr for attr in dir(labeller) if not attr.startswith('__')]

print("Available functions and classes:")
print(available_attributes)

from google.colab import files
import os

# Clean up any old attempts
!rm -rf /content/dataset
!rm -rf /content/final_dataset

print("Please upload your new 'my_labeled_data.zip' file.")
uploaded = files.upload()
zip_name = next(iter(uploaded))

# Unzip into /content/dataset
!unzip -q {zip_name} -d /content/dataset
print("\n‚úÖ Manual dataset uploaded and unzipped successfully!")

# Clean up any old attempts
!rm -rf /content/dataset
!rm -rf /content/final_dataset
!rm -rf /content/Labellerr_Assignment

# This command downloads a complete, correctly formatted dataset
# with images and labels for both training and validation.
!pip install roboflow -q

from roboflow import Roboflow
# Note: This uses a public key for a public dataset.
rf = Roboflow(api_key="YOUR_API_KEY")
project = rf.workspace("new-workspace-g28a8").project("car-segmentation-sjevb")
dataset = project.version(1).download("yolov8")

# The dataset is now downloaded and unzipped in a folder named 'car-segmentation-1'
# Let's rename it to 'dataset' to match our training script
!mv car-segmentation-1 dataset

!pip install roboflow -q
from roboflow import Roboflow

# --- YOUR ACTION HERE ---
# After generating a NEW key, paste it here.
# DO NOT share your new key.
YOUR_NEW_SECURE_API_KEY = "PASTE_YOUR_NEW_SECURE_API_KEY_HERE"
# ------------------------

rf = Roboflow(api_key=YOUR_NEW_SECURE_API_KEY)
project = rf.workspace("new-workspace-g28a8").project("car-segmentation-sjevb")
dataset = project.version(1).download("yolov8")

# Rename the folder to 'dataset' to match our training script
!mv car-segmentation-1 dataset

print("\n‚úÖ Dataset downloaded successfully!")

!pip install roboflow -q
from roboflow import Roboflow

# --- YOUR ACTION HERE ---
# Paste your NEW, SECURE Roboflow API key here.
YOUR_NEW_SECURE_API_KEY = "PASTE_YOUR_NEW_SECURE_API_KEY_HERE"
# ------------------------

# --- NEW, WORKING DATASET ---
# This is a different public dataset that is confirmed to be available.
rf = Roboflow(api_key=YOUR_NEW_SECURE_API_KEY)
project = rf.workspace("mohamed-soliman-40a1v").project("car-parts-segmentation-gn73u")
dataset = project.version(1).download("yolov8")

# Rename the downloaded folder to 'dataset' to match our training script
!mv "Car-Parts-Segmentation-1" dataset

print("\n‚úÖ New dataset downloaded successfully!")

!pip install roboflow -q
from roboflow import Roboflow

# --- YOUR ACTION HERE ---
# Paste your NEW, SECURE Roboflow API key here.
YOUR_NEW_SECURE_API_KEY = "PASTE_YOUR_NEW_SECURE_API_KEY_HERE"
# ------------------------

# --- NEW, WORKING DATASET ---
# This is a different public dataset that is confirmed to be available.
rf = Roboflow(api_key=rf_5mKbVGQq7SadAXR0eKQeDUJmVQo1)
project = rf.workspace("mohamed-soliman-40a1v").project("car-parts-segmentation-gn73u")
dataset = project.version(1).download("yolov8")

# Rename the downloaded folder to 'dataset' to match our training script
!mv "Car-Parts-Segmentation-1" dataset

print("\n‚úÖ New dataset downloaded successfully!")

!pip install roboflow -q
from roboflow import Roboflow

# --- YOUR ACTION HERE ---
# Paste your NEW, SECURE Roboflow API key here.
YOUR_NEW_SECURE_API_KEY = "rf_5mKbVGQq7SadAXR0eKQeDUJmVQo1"
# ------------------------

# --- NEW, WORKING DATASET ---
# This is a different public dataset that is confirmed to be available.
rf = Roboflow(api_key=YOUR_NEW_SECURE_API_KEY)
project = rf.workspace("mohamed-soliman-40a1v").project("car-parts-segmentation-gn73u")
dataset = project.version(1).download("yolov8")

# Rename the downloaded folder to 'dataset' to match our training script
!mv "Car-Parts-Segmentation-1" dataset

print("\n‚úÖ New dataset downloaded successfully!")

!pip install roboflow -q
from roboflow import Roboflow

# --- YOUR ACTION HERE ---
# Paste your NEW, SECURE API key inside the quotation marks.
# Do not use the old one.
YOUR_NEW_SECURE_API_KEY = "usQeiot8ARp9nqWL9olh"
# ------------------------

# --- NEW, WORKING DATASET ---
rf = Roboflow(api_key=YOUR_NEW_SECURE_API_KEY)
project = rf.workspace("mohamed-soliman-40a1v").project("car-parts-segmentation-gn73u")
dataset = project.version(1).download("yolov8")

# Rename the downloaded folder to 'dataset' to match our training script
!mv "Car-Parts-Segmentation-1" dataset

print("\n‚úÖ New dataset downloaded successfully!")

!pip install ultralytics

from ultralytics import YOLO

# start training
model = YOLO("yolov8n-seg.pt")  # small YOLOv8 segmentation model
model.train(data="project/data.yaml", epochs=100, imgsz=640)

from google.colab import files
files.upload()  # select project.zip

# List the contents of the project folder
!ls /project

from google.colab import files
files.upload()  # select a ZIP file of your folder

!unzip export-#YJ27LdqKY6zGfhTbw30H.zip -d /content/

from google.colab import files
files.upload()  # select a ZIP file of your folder

!unzip export-#YJ27LdqKY6zGfhTbw30H.zip -d /content/

!ls /content/export-#YJ27LdqKY6zGfhTbw30H

!pip install ultralytics

from ultralytics import YOLO

# Load the small YOLOv8 segmentation model
model = YOLO("yolov8n-seg.pt")

# Train the model
model.train(
    data="/content/export-#YJ27LdqKY6zGfhTbw30H/data.yaml",
    epochs=100,
    imgsz=640
)

from ultralytics import YOLO
model = YOLO("/content/runs/segment/train2/weights/best.pt")

results = model.predict("/content/export-#YJ27LdqKY6zGfhTbw30H/images", save=True)

import shutil
shutil.make_archive("predictions", 'zip', "/content/runs/segment/predict")

from google.colab import files
files.download("predictions.zip")

import pandas as pd

df = pd.read_csv("/content/runs/segment/train/results.csv")  # adjust "train" or "train2" if needed
print(df.tail())  # shows the last few epochs

import pandas as pd

df = pd.read_csv("/content/runs/segment/train2/results.csv")  # change to your folder name
print(df.tail())  # shows last few epochs

!pip install ultralytics
!pip install opencv-python-headless

from ultralytics import YOLO

# Load the trained model from train2
model = YOLO("/content/runs/segment/train2/weights/best.pt")

# Track objects in your demo video
results = model.track(
    source="/content/demo_video.mp4",
    tracker="bytetrack.yaml",
    show=True,
    save=True,
    save_txt=True
)

from ultralytics import YOLO

# Load the trained model from train2
model = YOLO("/content/runs/segment/train2/weights/best.pt")

# Track objects in your demo video
results = model.track(
    source="/content/demo_video.mp4",
    tracker="bytetrack.yaml",
    show=True,
    save=True,
    save_txt=True
)

import cv2

input_video = "/content/demo_video.mp4"
output_video = "/content/demo_video_cropped.mp4"

cap = cv2.VideoCapture(input_video)
width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps    = cap.get(cv2.CAP_PROP_FPS)

# Define crop rectangle (x, y, w, h) - adjust as needed
x, y, w, h = 0, 100, width, height-200  # example: crop top & bottom

# Resize to smaller resolution (optional)
new_w, new_h = 384, 640

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video, fourcc, fps, (new_w, new_h))

while True:
    ret, frame = cap.read()
    if not ret:
        break
    cropped = frame[y:y+h, x:x+w]
    resized = cv2.resize(cropped, (new_w, new_h))
    out.write(resized)

cap.release()
out.release()
print("Cropped & resized video saved:", output_video)

from ultralytics import YOLO

# Load the trained model from train2
model = YOLO("/content/runs/segment/train2/weights/best.pt")

# Track objects in your demo video
results = model.track(
    source="/content/demo_video_cropped.mp4",
    tracker="bytetrack.yaml",
    show=True,
    save=True,
    save_txt=True
)